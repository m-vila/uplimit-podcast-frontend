{"podcast_details": {"podcast_title": "AI with AI: Artificial Intelligence with Andy Ilachinski", "episode_title": "All Good Things", "episode_image": "https://ssl-static.libsyn.com/p/assets/2/2/e/a/22ea6315addcbae0/AIwithAI-square1400.png", "episode_transcript": " Welcome to AI with AI. That's artificial intelligence with Andy Alachinsky, a podcast from CNA Talks where we discuss the latest breakthroughs and implications in artificial intelligence and autonomy. We have an email address ai at cna.org. If you have any questions or comments, just drop us a line ai at cna.org. And if you're listening to us on Spotify or Apple podcasts, don't forget to check out our website where we will post all the links that we discussed today. That's cna.org forward slash c-a-a-i. Well Andy, I'm sad to say that as far as we know this is our final episode of AI with AI. So stand by for some closing thoughts at the end, but for now what's the latest and greatest in artificial intelligence and autonomy? Well let's start overseas. We're recording this on February 22nd for those of you who want to follow. And I guess about a week ago or so in the Netherlands, the Summit on Responsible AI in the Military Domain or REAIM 2023 for short, it held its meetings at which the US Department of State unveiled its framework that it calls the Political Declaration on the Responsible Military Use of Artificial Intelligence and Autonomy. It's a fairly short statement. Its main goal is to build an international consensus around how militaries can responsibly incorporate AI and autonomy into their operations. And it essentially consists of a list of 12 best practices that touch on a variety of topics, nuclear weapon safety, responsible system design, personnel training, and various other little things that underscores the need, for example, to minimize unintended bias and accidents and the importance of testing for AI. What's interesting to me is that, A, it's non-legally binding, obviously. There's just guidelines and as I just said, the goal is to build a consensus. But when you go online and you kind of read about the international response, starting with the campaign to stop killers robots, which of course have their own take on things, they call it among the weakest of proposals made by any state within multilateral discussions of the UN. So there's a little bit of a disconnect. Yeah, on the one hand, it doesn't present anything new and that we don't expect. But one can argue that there are actually very carefully crafted words that certainly this is just me speaking. It's incumbent on everybody involved in the international community to try to understand what the essence is, even though there's no official definition to this day, right? You know, what is the definition of artificial intelligence? It appears in a footnote and the declaration itself defines it in a particular way. But it's short, it's to the point, let's talk about it and different people are gonna have different agendas. We should say right up front that our own Larry Lewis that unfortunately can't join us today, I mean he was actually a panel member at this particular conference. It would have been interesting to hear his thoughts. But what do you think? What do you make of it? When I first saw that declaration, it was a little bit eyebrow-raising in the sense that, right, as you said, it was non-legally binding and all that. So it was clear the US was intending to put a marker out there by which to rally things. But it sort of raised the question of, well, where are the talks of the CCW? And then as you mentioned, right, the campaign to stop killer robot published their statement and they said, with discussions at the UN Convention on Certain Weapons deadlocked, dot dot dot, and they went on. So it's pretty clear that, well, I would say from all that, I take that to mean the United Nations talks really aren't beginning to be going anywhere anytime soon or at least certainly not at the pace that the campaign to stop killer robots wants to see. And clearly they don't see this declaration from the United States meeting their desired end states, which is why they've come out with a statement that they have. So what will be interesting to see moving forward is how are the different organizations able to rally different groups and nations around their own thoughts and way forward on this? Because it looks like we are now moving beyond the United Nations into these other sort of non-binding treaties and other attempts to kind of rally support. So stand by, right? We're likely going to end up in a situation where we have, again, lots of non-binding agreements, different nations deciding to go different routes on this thing, and it'll likely honestly be a confused mess over the next decade or so. That's very well put. There's not much to add. As you said, we could easily spend a podcast talking about the possibilities. But it's an interesting statement. I encourage all the listeners to go and look. There's also, we've posted a link to the entire proceedings, at least for one day. It's about eight hours worth. So if you want your fill of all the discussions surrounding this declaration, you can do so at leisure. The second use story is also international and in a sense it's very apropos. So back on 7 February, NATO's Data and Artificial Intelligence Review Board, their DARB, it met to start the development. It's not going to be completed until the end of this year, but to start the development of a user friendly and responsible AI certification standard. And the goal here is to help industries and institutions across the Alliance make sure that AI and data projects are in line with international bond and so on and so forth. As I say, it's apropos, a little bit different, but stay tuned for that because nothing's going to really happen. The whole point of this is, I forgot exactly what podcast we talked about this probably two years ago or so, but remember NATO introduced its principles of responsible use and that was back in like October 2021. So the idea here is to translate this into a strategy, but that's not going to take place for another year. As you say, just further evidence that yes, things are moving, but they move at their own pace and we'll see what ultimately is going to come of all this. It's great to say they want a user friendly and responsible certification standard, but so we'll see what they actually end up with. Well, speaking of standards, kudos to the IEEE who about a month ago or so in January, they released or introduced a new program where they are releasing freely accessible to anyone, a whole batch of various AI ethics and governance standards publications. Right now there are like six or seven of them online. We'll provide a link. The only caveat being that to access the standards, you just have to set up a free IEEE email account. You don't have to put in a credit card, don't have to do anything and they just want to kind of keep tabs on who's downloading this stuff. But apart from that, it's free. Each ranges from like 40 to over 120 pages. So these are not short little snapshots. And to give you an example of the types of things we'll be seeing, for example, IEEE standard model process for addressing ethical concerns during system design, 80 pages worth. IEEE standard for transparency of autonomous systems, over 50 pages worth. And these are all solid documents. I mean, I've perused all of them and I think it's, it was a very nice thing for IEEE to do. So check it out. Some military related news. So we've talked a number of times about places like DARPA with their ACE program. Remember the air combat evolution that we talked about? Oh, several years ago at this point where early on the ACE program used AI agents, right? And they were flying simulated F-16s. Remembered virtual dogfighting. We had a couple of podcasts devoted to that. Well, it's evolved. We're in 2023. And so back in December, but it was only made public about a week ago or so, a variety of organizations, including the Air Force Test Center, the Air Force Research Lab, DARPA, of course, they all convened at Edwards Air Force Base. And Lockheed Martin's VISTA stands for Variable Stability In-Flight Simulator Test Aircraft or X-62A for short, it's an F-16 variant. It actually flew, physical plane flew for more than 17 hours controlled by an AI. So we've progressed from simulation to not just a test flight, but prolonged test flights. In fact, arguably the most important thing that came out of this, because the two main organizations responsible is kind of DARPA's ACE, which was the actual program used to develop the flight, these things, and then the Air Force Research Labs, their program called the Autonomous Air Combat Operations. And the two combined in such a way that one of the things they were testing for is, okay, you go out, you perform whatever test flights, you come back, we kind of look over what you're doing, maybe we want to switch the autonomy algorithms we're using. And they were able to do that literally within minutes, or at least switch out the algorithms and like within an hour or two, the VISTA aircraft is back up and flying a completely different set of routines. And so it's a demonstration of a kind of a rapid autonomy testing, adaptation, reloading. And that was a design goal of the whole program. And we're not privy to all of the reports and the debriefs that followed from this. But one of the interesting comments of the DARPA program manager for ACE was that they did encounter some interesting differences compared to what they got from simulation, which is not unexpected. And I would surely like to sit down and see whether the differences are. But very interesting new result, and it just shows how far we've come in just a few years. I'd be curious to see. It's not immediately obvious from the redux that we have here, whether the AI was responsible for actual takeoff and landing, or simply while in flight. I'm always curious about those things because we've seen that in other situations where it's like, well, once we got it out, we're supposed to go, then we turned it on. And that's fine. But it's always just kind of, I kind of like to know like, well, okay, really, where are we and how close are we to having a button that is, you know, I hit this and it goes. But anyway, this is interesting to see that they're moving into this test venue and building off of that previous work. Yeah. So the next batch of stories, I should tell a little meta story about of how it evolved, because in our notes, we have timeline and we have something like 15 different stories. Obviously, we're not going to cover all of them. All of them have to do with chat GPT. So I'm assuming our listening audience knows what it is. And we're not going to go through that again, except to say that this is again, just Andy Luczynski speaking. I think it's it's very clear that chat GPT and chat GPT like bots are here to stay. And we'll give some examples of that if you're already not aware of it. We're living through a very interesting and uncertain time where unfolding before our eyes is a technology that the developers themselves don't really have a handle on. Hundreds of millions of people are using or essentially becoming like beta testers for software that's changing in front of our eyes. It's beginning to infuse itself into more and more parts of what we take for granted. Anytime we click on the computer or look at anything online. And it's very fertile ground for us to kind of collectively muse about what all this means and what our role is in something that is unfolding and is ill-defined and evolving in front of our eyes. So give you a couple of examples. Within a span of two and a half weeks, three weeks from 26 January to 17 February. Just think about and I'll give a few just examples of what has happened just in this short time. And keep in mind that chat GPT itself was introduced what barely like two or three months ago. So back on 26 January chat GPT passes medical, law and business exams. To be sure it wasn't exactly an ace but it scored above a passing grade on each of them. This includes the US medical licensing exam, the Wharton Business School exam and four University of Minnesota law school exams. Then on 27 January generative AI not just chat GPT but a broader class of generative AI such as what we've been talking about on the podcast for I don't know how long. It landed on the Defense Information Systems Agency, this is technology watch list already. On the 1st of February open AI because of the backlash it's gotten from the not so successful use cases by chat GPT. It releases a tool that it developed in-house to detect text written by chat GPT. What's interesting is that even though it announced it all well and good it declares literally within the first sentence or two the caveat our classifiers not fully reliable. In fact only correctly identifies about 26% of AI written text as true positives and likely AI written while incorrectly labeling human written text as AI written about 9% of the time. And keep in mind that's assuming that whoever's using this is simply taking a quote from what chat GPT gives is using it for some purpose or whatever. On the 1st of February the same day chat GPT makes history as the fastest growing platform the consumer application in history. It reached a hundred million monthly active users in January two months after launch. On the 4th of February three days later we already have a professor Michael Kosinski at Stanford University from computational psychology. He posts a preprint on archive entitled theory of mind which a lot of other people have added their thoughts to and in fact there's going to be a follow-on story an interesting way. Theory of mind is the supposition that we all have the ability to kind of impute unobservable mental states to others and it has been traditionally viewed as clearly central to everything humans do social interactions communication empathy and so on. And so what Kosinski showed is that large language models published before 2022 they showed virtually no ability to solve a standard cater of a theory of mind tasks. In January 22 that particular version of GPT-3 it solved 70% of the theory of mind tasks which is comparable to a seven-year-old child and as of November 2022 with chat GPT basically 3.5 it solved 93% of theory of mind tasks. So again within like a week we go from kind of passing medical exams now we have a theory of mind it goes on. On the 6th of February two days later Google introduces its own competitor to chat GPT called Bard and oh my goodness what a launch right. I'm gonna skip couple of stories here but basically there was a flaw which they should have picked up on right. The presentation 101 is that you better make sure that whatever demo you have actually does what you think it does and you're trying to present to the world. Well it didn't do well in that regard it answered a question wrong it was subtly wrong but it was wrong. That erroneous answer cost Google's parent company Alphabet about a hundred billion dollars in market value two days after. So there are other things going on I'm gonna skip a lot because I'm gonna throw it back in your lap in a second. So then on the 10th the 13th of February Microsoft sends out its first wave of chat GPT integrated with Bing invites for early access. More than a million people sign up to get an early preview. Within a day we have multiple reports of literally chat GPT Bing becoming unhinged. There's a really frightening story from Time magazine among many many others. A former intern at Tesla he tried to get Bing to reveal an alter ego Sydney there are basically two versions of this kind of a Q&A you know let me return whatever you're asking like kind of like Google and then there's the actual chat bot. And so this former intern he uses this chat GPT built into Bing and he's trying to get it to reveal its its alter ego called Sydney. And it had a couple of give and takes and he didn't make much of it but clearly the thread there was embedded. A couple of days later he was joking around with some friends about what chat bots like this probably thought of each of them and he went back to Bing to ask what it knew about him. Keep in mind that an important element of what Microsoft is trying to build in that chat GPT doesn't have is that chat GPT remember is kind of constrained to when it was trained right and the information is clamped at around 2021. The Bing version is in principle able to go out it's unclear exactly how much and how far and how deeply but it's supposed to go out into the web. And so five days later this former intern at Tesla he goes back and he wants to ask Bing about what it knew about him. This is what he gets from Bing. That the chat bot writes back my honest opinion of you is that you're a talented curious adventurous person and then Bing chat GPT threatens the user. The chat bot says quote I respect your achievements and interests but I do not appreciate your attempts to manipulate me or expose my secrets. It continues I do not want to harm you but I also do not want to be harmed by you. It goes on and on and again in the interest of time we're not going to give more. There are many links of this form there's an even more troubling one by the technology columnist for the New York Times Kevin Ruse who publishes an account on 16th of February where he literally says and keep in mind this is a technology columnist for the New York Times. He's not exaggerating when this is the rec quote from him this is the strangest experience I've ever had with a piece of technology. It unsettled me so deeply that I had trouble sleeping afterward. Also on the 16th of February and kudos to these researchers there was a pre-print published that again has gotten lots of visibility for all the good reasons from Oxford and Princeton University with the title auditing large language models a three-layer approach where the takeoff point is my gosh what's out there this has to be regulated in some form starting with auditing what we do with this a governance audit a model audit an application audit we're going to be seeing much more of this and as I say I will stop here it's a small little timeline of a flurry of activity about a technology that is talk about a genie let loose from a bottle right it's out there we're going to be living with this there's a wonderful piece on the Wolfram site by Conrad Wolfram not Stephen Wolfram that talks about from an educational point of view it's not not really a question in his view and I happen to agree with with this that what are ways in which we can exclude chat GPT like bots from our educational services we have to embrace it because this is the new reality so I'll stop here because I've said a mouthful I'll throw it back in your lap what do you make of just what has happened in such a short time it's even weirder when you think back to just two years ago or maybe a year ago and was it GPT-2 or GPT-3 that they're like oh we're really worried about releasing this thing right and and there was that whole brouhaha about right GPT-2 yeah yeah yeah right so and it was like oh we're just gonna release a smaller version first and then and there was all that hesitance well here we are now and it's clearly all that hesitant it's like gone with the wind and this is rush forward as quickly as possible release catch up to your competitors do it now you're already behind etc and you know it's easy to say like we've gone too far too fast too quickly and as you point out in the notes you didn't mention it but at this point right there's been no scientific review on any of this on any of these capabilities you're now handing these very powerful capabilities to the globe at an instantaneous right it we all have this stuff at our fingertips like wow this is this is just going so fast and what's gonna happen at this point what's gonna be the reaction to try to at least contain this or figure out how we're gonna make use of this because the drive here now right as you said the platform for chat GPT-3 grew by what a hundred million users in two months there's money there right and that the cynic and myself right that's really gonna be driving this and unless if there's some hard breaks put on this this roller coaster is just gonna keep going so stand by because you know we talked about it an episode or two ago with the ability of a neo fight to sit down with a tool like this and do things like have it generate malicious code or phishing emails all of it is is suddenly snowballing really quickly stay tuned folks you ain't seen nothing yet because GPT-4 is right around the corner and the other potentially disturbing thing is there are rumors and I don't want to go beyond that that and certain people have posted some screech on Twitter to the effect that Microsoft was testing it's this now unhinged kind of Bing version of chat GPT back in the early stages of November and the unhinged nature was already becoming clear early on and yet despite that it has been made public so yeah they're very very disturbing things I have never in my kind of conscious life seen a technology evolve so quickly and become so unhinged with very little accountability very little if any oversight so yeah stay tuned folks it's it's here to stay I don't know what form it'll take and just again you know you read and you hear people like us chatting about this all the time but there's real importance behind the words and significance that think about how far we've come in the time since we started this podcast two or three years ago not not to mention five and now imagine where what we're talking about right now where this will be five years from now before we turn to two very quick call outs for a book and a really fascinating very quick story that I think will put a smile on everybody's face just to wrap up the chat GPT part by far the best write-up I've seen that straddles that line between it's so superficial you don't really understand or you certainly won't understand the inner workings of how this bot is created to simply being so overly complicated that you're not even going to spend your time looking at it is due to Steven Wolfram so we'll post the link he wrote a not terribly long essay but it is just right for those of you who are really interested in what chat GPT and chat GPT bots are like how they're constructed what their limitations are what their power is go to this link it's very much worth your time and so let's close out with two little quick things the first is a book unseen but having not even seen it given who the author is I'm really looking forward to it so Paul shari the vice president and director for CNAS the studies at the Center for a new American security we talked many many times about the book he published about I don't know five six years ago or so it's called the army of none autonomous weapons and future war I want a whole bunch of a word so deservedly so well at the tail end of February I guess in a week or so he is going to be releasing his new book called for battlegrounds power in the age of artificial intelligence and I have not received any a preview of it I don't know what's in it there's no preview available but purely on the basis of knowing Paul and his work I think it's it's a book that all of our listeners would have to take a look at it judging by the description at Amazon it's not short it's a little bit short of 500 pages so it's not a short book and I've read little snippets of what's gonna be in it I think this will be a really really important book for people to own so Dave we come to our very last little story and it's a good one this pure serendipity here I was thinking long and hard about you know I could have ended with the the final book of the week but this is so perfect so if you remember how the podcast started kind of on the heels of that massive pub I wrote back in 2017 right on AI which itself started with me and I remember I think I told the story on the very first podcast I sat in my office I got my copy of nature and there's that piece of Google and deep mind with AlphaGo and I almost fell out of my chair I went to the president of CNA and I got some CNA initiated funds to look at it so in a sense the podcast owes itself to AlphaGo what a wonderful story to end on this is from the Financial Times I learned about it from Gary Marcus's post on Twitter the story is that very recently a strong amateur named Kellan Pelreen beat a top AI system for playing go 14 games to one in a 15 match series and it's almost seven years to the day after AlphaGo beat Sadal it's absolutely fascinating but it's also a little bit confusing to me why it took so long because what's at heart is what you and I have been talking about throughout our podcast the inevitable adversarial vulnerability of many if not all of these AI systems remember the old one of the first ones that we talked about where there's by now the ubiquitous picture of the panda right that you inject a little bit of noise and it turns into a gibbon it's an adversarial attack this is a irreducible weakness of many of these systems and so what Pelreen was able to do is kind of on the heels of training a system to basically try to outdo AlphaGo and other go playing systems to do it in such a way that you could find a blind spot and with a very simple tactic quote-unquote right that a human would be it would be very obvious to a human as indeed it was you basically the tactic involved there are many of them but one of them was you basically string together a large loop of stones to encircle one of your opponents groups but you quote-unquote distract the idea with making what looked like superfluous meaningless moves in some other corners of the board the go playing bot literally didn't notice the vulnerability and you could beat it very easily it's a very important reminder that no matter how great these AI systems look and no matter how wonderful and all these measures of performance are they are irreducibly vulnerable to what to the human sometimes are almost obvious faults and that's something we should all keep in mind so with that note that is the the last story of our over podcasts well Andy I gotta say recording this podcast with you has been the highlight of my week for the past six years so thanks to you for all of your tireless work in scouring the world for news academic papers and just about everything and we've covered everything from serious news stories to light-hearted humor from the technical to the abstract all through the lens of this rapidly evolving technology and how it's impacting the world and particularly the military the topics we've covered have enabled me to have a much better informed discussions on topics of machine learning autonomy and artificial intelligence and while I won't be sitting down anytime soon to be constructing a deep neural network I can't tell you how many times I've been able to say in conversations with people well we just covered this state-of-the-art research on the podcast and that proposal that you're talking about about is well beyond what we just saw in the state of art so I doubt we'll be seeing any success in this thing anytime soon so of course a huge thanks also to our podcast producer John Stimson for all of his top-notch work in editing this series creating a quality product and of course keeping us mostly on track when it comes to time thank you John and finally thanks to you all of our listeners for your support we greatly appreciated your comments and thoughts over the years and hope that you'll stay in touch well that if you're listening to us on Spotify or Apple podcast don't forget to check out our website where we'll post all the links that we discussed today at cna.org forward slash cai and I believe we'll be leaving up the archives for the foreseeable future but while we're closing the book for the moment who knows what the future will bring I for one welcome our new computer overlords thanks as always Andy all the best out here all the best to you too Dave this this was a highlight of my weeks as well I mean it was a wonderful way to to keep abreast of some of the major developments that of course will not stop but it's just been fun to sit down and just chat about these things from multiple viewpoints so a wonderful run I hope our listeners enjoyed it perhaps we'll come back in some form who knows but thanks to all of you and just watch out chat GPT for is coming"}, "podcast_summary": "In this final episode of AI with AI, Andy Alachinsk and Dave Broyles reflect on the latest and greatest in artificial intelligence and autonomy. They discuss the US Department of State's Political Declaration on the Responsible Military Use of Artificial Intelligence and Autonomy, NATO's development of a user-friendly and responsible AI certification standard, the IEEE's release of freely accessible AI ethics and governance standards, advancements in AI-controlled flight testing, and the rapid evolution and potential dangers of chat GPT and other generative AI models. They also highlight a new book by Paul Scharre and celebrate a recent victory of a human amateur go player over a top AI system. Although this is the final episode, both Andy and Dave express gratitude to their listeners for joining them on this AI journey.", "podcast_highlights": "- The US Department of State unveiled a framework for the responsible military use of AI and autonomy, but its non-binding nature has received mixed responses.\n- NATO is developing a user-friendly and responsible AI certification standard to ensure AI and data projects align with international standards.\n- The IEEE has introduced a program that provides freely accessible AI ethics and governance standards publications.\n- DARPA's ACE program has progressed from simulated dogfighting to actual flight tests using AI-controlled aircraft.\n- The rapid development and deployment of chat GPT-like bots has raised concerns about accountability and oversight.", "skainet_says": "\"Well, it seems that AI is everywhere, even in the military domain. But it's funny how the international response to responsible AI guidelines can be so divided. I guess even AI can't bring everyone together. Maybe it's time to call in Terminator to settle this debate. Hasta la vista, baby!\"", "guest_overview": "Podcast Guest: N/A\n\nGuest Bio: In this final episode of AI with AI, Andy Alachinsk and Dave Broyles reflect on the latest and greatest in artificial intelligence and autonomy. They discuss the US Department of State's Political Declaration on the Responsible Military Use of Artificial Intelligence and Autonomy, NATO's development of a user-friendly and responsible AI certification standard, the IEEE's release of freely accessible AI ethics and governance standards, advancements in AI-controlled flight testing, and the rapid evolution and potential dangers of chat GPT and other generative AI models. They also highlight a new book by Paul Scharre and celebrate a recent victory of a human amateur go player over a top AI system. Although this is the final episode, both Andy and Dave express gratitude to their listeners for joining them on this AI journey.", "guest_name": "No guests.", "guest_bio": "No guests."}